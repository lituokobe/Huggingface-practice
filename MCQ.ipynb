{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62ac9c79-1217-474f-a80d-66477d39be99",
   "metadata": {},
   "source": [
    "# Multiple Choice Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f54bd67-6b3f-4df1-bef0-8c48000460cf",
   "metadata": {},
   "source": [
    "### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a32bbf7-e9a7-4178-be65-8b09605b52b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4c5d47-c503-44ce-9075-c3cda416a259",
   "metadata": {},
   "source": [
    "### 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80095c02-ec6c-4591-b958-2df106e3677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c3 = DatasetDict.load_from_disk(\"../c3/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a282a9-07b0-4978-8dc9-7a21edf3cea6",
   "metadata": {},
   "source": [
    "`DatasetDict.load_from_disk(\"\")`\n",
    "Purpose: Load a dataset (or dataset dictionary) that was previously saved locally with `.save_to_disk()`.\n",
    "\n",
    "This method directly restores the dataset exactly as it was saved, including:\n",
    "\n",
    "- Column formats\n",
    "- Features schema\n",
    "- Train/validation/test splits (if it was a DatasetDict)\n",
    "\n",
    "No processing is done — it just reopens the existing preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9acc6305-ced3-404d-98b5-193de8299867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['id', 'context', 'question', 'choice', 'answer'],\n",
       "        num_rows: 1625\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['id', 'context', 'question', 'choice', 'answer'],\n",
       "        num_rows: 11869\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'context', 'question', 'choice', 'answer'],\n",
       "        num_rows: 3816\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "276d5a77-75b3-4083-a149-fb292d67dd77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': [3, 4],\n",
       " 'context': [['男：我记得你以前很爱吃巧克力，最近怎么不吃了，是在减肥吗?', '女：是啊，我希望自己能瘦一点儿。'],\n",
       "  ['女：过几天刘明就要从英国回来了。我还真有点儿想他了，记得那年他是刚过完中秋节走的。',\n",
       "   '男：可不是嘛!自从我去日本留学，就再也没见过他，算一算都五年了。',\n",
       "   '女：从2000年我们在学校第一次见面到现在已经快十年了。我还真想看看刘明变成什么样了!',\n",
       "   '男：你还别说，刘明肯定跟英国绅士一样，也许还能带回来一个英国女朋友呢。']],\n",
       " 'question': ['女的为什么不吃巧克力了?', '现在大概是哪一年?'],\n",
       " 'choice': [['刷牙了', '要减肥', '口渴了', '吃饱了'],\n",
       "  ['2005年', '2010年', '2008年', '2009年']],\n",
       " 'answer': ['要减肥', '2010年']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3[\"train\"][3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6338db2-41a3-476d-a987-07c2f4a8353f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': [3, 4],\n",
       " 'context': [['这几年公司发展得很不错，每年春节前都会发给工人两个月的奖金，但是今年公司却没挣到多少钱。经理很担心工人们会伤心、失望。这天，他突然想起小时候去买糖：别的服务员都是先抓一大把，拿去称，再一颗一颗减少；只有一个服务员，每次都抓不够重量，然后一颗一颗往上加。虽然拿到的糖是一样的，但人们都喜欢后者。经理想到了办法。过了两天，传来一个消息——今年公司发展不好，有些人可能得离开公司。工人们听了之后都开始担心，以为要离开的是自己。后来经理宣布了一个消息：大家都是一家人，虽然公司有困难，但不能丢掉任何人，只是没有奖金了。这个消息使所有的人都放下了心：奖金不重要，有工作就好。春节快到了，工人们都做了过个穷年的打算。这时经理通知开会，工人们又担心：“会有什么变化吗？”谁知参加会议的人回来兴奋地喊道：“有！有！还是有奖金的！一个月的！”工人们听了，发出一片热烈的欢呼声。'],\n",
       "  ['这几年公司发展得很不错，每年春节前都会发给工人两个月的奖金，但是今年公司却没挣到多少钱。经理很担心工人们会伤心、失望。这天，他突然想起小时候去买糖：别的服务员都是先抓一大把，拿去称，再一颗一颗减少；只有一个服务员，每次都抓不够重量，然后一颗一颗往上加。虽然拿到的糖是一样的，但人们都喜欢后者。经理想到了办法。过了两天，传来一个消息——今年公司发展不好，有些人可能得离开公司。工人们听了之后都开始担心，以为要离开的是自己。后来经理宣布了一个消息：大家都是一家人，虽然公司有困难，但不能丢掉任何人，只是没有奖金了。这个消息使所有的人都放下了心：奖金不重要，有工作就好。春节快到了，工人们都做了过个穷年的打算。这时经理通知开会，工人们又担心：“会有什么变化吗？”谁知参加会议的人回来兴奋地喊道：“有！有！还是有奖金的！一个月的！”工人们听了，发出一片热烈的欢呼声。']],\n",
       " 'question': ['今年公司怎么样？', '根据经理小时候买糖的情况，可以知道？'],\n",
       " 'choice': [['发展得不错', '挣的钱不多', '要给工人发糖', '要发两个月的奖金'],\n",
       "  ['服务员给的重量不够', '服务员一块儿一块儿地卖', '服务员卖糖的方式都一样', '不同的卖糖方法影响人的心情']],\n",
       " 'answer': ['', '']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3[\"test\"][3:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea8870f-827f-4aff-bb05-7179cc027e91",
   "metadata": {},
   "source": [
    "Test data doesn't have values in 'answer'. In the preprocessing of data, we remove \"test\" first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eac5271c-2786-430e-8f9f-480f74cff3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'context', 'question', 'choice', 'answer'],\n",
       "    num_rows: 1625\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3.pop(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eb25755-f37c-45a5-a726-f6f49aae6cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'context', 'question', 'choice', 'answer'],\n",
       "        num_rows: 11869\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'context', 'question', 'choice', 'answer'],\n",
       "        num_rows: 3816\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc34da15-876c-41b4-8c82-d0c7cfbe4b1e",
   "metadata": {},
   "source": [
    "### 3. Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5af419d5-f664-4823-b739-4eb73cd47a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36d6a831-bed2-4244-aac4-a45e73cd4407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='hfl/chinese-macbert-base', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36792785-fb8b-47a1-8b2d-7724fec182db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 10,\n",
       " 'context': ['女：每次考试之前我都睡不着，所以考试的时候精神不好。', '男：那是因为你太紧张了，你在睡觉之前听点儿轻松的音乐吧。'],\n",
       " 'question': '关于女的，可以知道什么?',\n",
       " 'choice': ['考得好', '睡不着', '很轻松', '觉得累'],\n",
       " 'answer': '睡不着'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd7d5814-47b6-4dac-97eb-2fe8ded2e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_function(examples):\n",
    "    # examples[\"context\", \"question\", \"choice\", \"answer\"]\n",
    "    context = [] #sentence1\n",
    "    question_choice = [] #sentence2\n",
    "    labels = []\n",
    "    for idx in range(len(examples[\"context\"])):\n",
    "        ctx = \"\\n\".join(examples[\"context\"][idx])\n",
    "        question = examples[\"question\"][idx]\n",
    "        choices = examples[\"choice\"][idx]\n",
    "        for choice in choices:\n",
    "            context.append(ctx)\n",
    "            question_choice.append(question + \" \" + choice)\n",
    "        if len(choices) <4:\n",
    "            for _ in range(4-len(choices)):\n",
    "                context.append(ctx)\n",
    "                question_choice.append(question + \" \" + \"不知道\")\n",
    "\n",
    "        labels.append(choices.index(examples[\"answer\"][idx]))\n",
    "\n",
    "    # 1000*4 = 4000 -> 4000*256\n",
    "    tokenized_examples = tokenizer(context, question_choice, truncation = \"only_first\", max_length = 256, padding = \"max_length\")\n",
    "    \n",
    "    # 1000*4*256\n",
    "    tokenized_examples = {k: [v[i: i+4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}\n",
    "    tokenized_examples[\"labels\"] = labels\n",
    "    \n",
    "    return tokenized_examples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44dc1c0c-e2bf-45f8-b567-244deb76e826",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = c3[\"train\"].select(range(10)).map(process_function, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a4d1061-f00c-4b28-a6df-82a469afae40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4, 256)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(res[\"input_ids\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85625d7e-d50a-4d59-9b64-d12591e45b37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': [3, 4], 'context': [['男：我记得你以前很爱吃巧克力，最近怎么不吃了，是在减肥吗?', '女：是啊，我希望自己能瘦一点儿。'], ['女：过几天刘明就要从英国回来了。我还真有点儿想他了，记得那年他是刚过完中秋节走的。', '男：可不是嘛!自从我去日本留学，就再也没见过他，算一算都五年了。', '女：从2000年我们在学校第一次见面到现在已经快十年了。我还真想看看刘明变成什么样了!', '男：你还别说，刘明肯定跟英国绅士一样，也许还能带回来一个英国女朋友呢。']], 'question': ['女的为什么不吃巧克力了?', '现在大概是哪一年?'], 'choice': [['刷牙了', '要减肥', '口渴了', '吃饱了'], ['2005年', '2010年', '2008年', '2009年']], 'answer': ['要减肥', '2010年'], 'input_ids': [[[101, 4511, 8038, 2769, 6381, 2533, 872, 809, 1184, 2523, 4263, 1391, 2341, 1046, 1213, 8024, 3297, 6818, 2582, 720, 679, 1391, 749, 8024, 3221, 1762, 1121, 5503, 1408, 136, 1957, 8038, 3221, 1557, 8024, 2769, 2361, 3307, 5632, 2346, 5543, 4607, 671, 4157, 1036, 511, 102, 1957, 4638, 711, 784, 720, 679, 1391, 2341, 1046, 1213, 749, 136, 1170, 4280, 749, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 4511, 8038, 2769, 6381, 2533, 872, 809, 1184, 2523, 4263, 1391, 2341, 1046, 1213, 8024, 3297, 6818, 2582, 720, 679, 1391, 749, 8024, 3221, 1762, 1121, 5503, 1408, 136, 1957, 8038, 3221, 1557, 8024, 2769, 2361, 3307, 5632, 2346, 5543, 4607, 671, 4157, 1036, 511, 102, 1957, 4638, 711, 784, 720, 679, 1391, 2341, 1046, 1213, 749, 136, 6206, 1121, 5503, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 4511, 8038, 2769, 6381, 2533, 872, 809, 1184, 2523, 4263, 1391, 2341, 1046, 1213, 8024, 3297, 6818, 2582, 720, 679, 1391, 749, 8024, 3221, 1762, 1121, 5503, 1408, 136, 1957, 8038, 3221, 1557, 8024, 2769, 2361, 3307, 5632, 2346, 5543, 4607, 671, 4157, 1036, 511, 102, 1957, 4638, 711, 784, 720, 679, 1391, 2341, 1046, 1213, 749, 136, 1366, 3951, 749, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 4511, 8038, 2769, 6381, 2533, 872, 809, 1184, 2523, 4263, 1391, 2341, 1046, 1213, 8024, 3297, 6818, 2582, 720, 679, 1391, 749, 8024, 3221, 1762, 1121, 5503, 1408, 136, 1957, 8038, 3221, 1557, 8024, 2769, 2361, 3307, 5632, 2346, 5543, 4607, 671, 4157, 1036, 511, 102, 1957, 4638, 711, 784, 720, 679, 1391, 2341, 1046, 1213, 749, 136, 1391, 7653, 749, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[101, 1957, 8038, 6814, 1126, 1921, 1155, 3209, 2218, 6206, 794, 5739, 1744, 1726, 3341, 749, 511, 2769, 6820, 4696, 3300, 4157, 1036, 2682, 800, 749, 8024, 6381, 2533, 6929, 2399, 800, 3221, 1157, 6814, 2130, 704, 4904, 5688, 6624, 4638, 511, 4511, 8038, 1377, 679, 3221, 1658, 106, 5632, 794, 2769, 1343, 3189, 3315, 4522, 2110, 8024, 2218, 1086, 738, 3766, 6224, 6814, 800, 8024, 5050, 671, 5050, 6963, 758, 2399, 749, 511, 1957, 8038, 794, 8202, 2399, 2769, 812, 1762, 2110, 3413, 5018, 671, 3613, 6224, 7481, 1168, 4385, 1762, 2347, 5307, 2571, 1282, 2399, 749, 511, 2769, 6820, 4696, 2682, 4692, 4692, 1155, 3209, 1359, 2768, 784, 720, 3416, 749, 106, 4511, 8038, 872, 6820, 1166, 6432, 8024, 1155, 3209, 5507, 2137, 6656, 5739, 1744, 5300, 1894, 671, 3416, 8024, 738, 6387, 6820, 5543, 2372, 1726, 3341, 671, 702, 5739, 1744, 1957, 3301, 1351, 1450, 511, 102, 4385, 1762, 1920, 3519, 3221, 1525, 671, 2399, 136, 8232, 2399, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1957, 8038, 6814, 1126, 1921, 1155, 3209, 2218, 6206, 794, 5739, 1744, 1726, 3341, 749, 511, 2769, 6820, 4696, 3300, 4157, 1036, 2682, 800, 749, 8024, 6381, 2533, 6929, 2399, 800, 3221, 1157, 6814, 2130, 704, 4904, 5688, 6624, 4638, 511, 4511, 8038, 1377, 679, 3221, 1658, 106, 5632, 794, 2769, 1343, 3189, 3315, 4522, 2110, 8024, 2218, 1086, 738, 3766, 6224, 6814, 800, 8024, 5050, 671, 5050, 6963, 758, 2399, 749, 511, 1957, 8038, 794, 8202, 2399, 2769, 812, 1762, 2110, 3413, 5018, 671, 3613, 6224, 7481, 1168, 4385, 1762, 2347, 5307, 2571, 1282, 2399, 749, 511, 2769, 6820, 4696, 2682, 4692, 4692, 1155, 3209, 1359, 2768, 784, 720, 3416, 749, 106, 4511, 8038, 872, 6820, 1166, 6432, 8024, 1155, 3209, 5507, 2137, 6656, 5739, 1744, 5300, 1894, 671, 3416, 8024, 738, 6387, 6820, 5543, 2372, 1726, 3341, 671, 702, 5739, 1744, 1957, 3301, 1351, 1450, 511, 102, 4385, 1762, 1920, 3519, 3221, 1525, 671, 2399, 136, 8166, 2399, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1957, 8038, 6814, 1126, 1921, 1155, 3209, 2218, 6206, 794, 5739, 1744, 1726, 3341, 749, 511, 2769, 6820, 4696, 3300, 4157, 1036, 2682, 800, 749, 8024, 6381, 2533, 6929, 2399, 800, 3221, 1157, 6814, 2130, 704, 4904, 5688, 6624, 4638, 511, 4511, 8038, 1377, 679, 3221, 1658, 106, 5632, 794, 2769, 1343, 3189, 3315, 4522, 2110, 8024, 2218, 1086, 738, 3766, 6224, 6814, 800, 8024, 5050, 671, 5050, 6963, 758, 2399, 749, 511, 1957, 8038, 794, 8202, 2399, 2769, 812, 1762, 2110, 3413, 5018, 671, 3613, 6224, 7481, 1168, 4385, 1762, 2347, 5307, 2571, 1282, 2399, 749, 511, 2769, 6820, 4696, 2682, 4692, 4692, 1155, 3209, 1359, 2768, 784, 720, 3416, 749, 106, 4511, 8038, 872, 6820, 1166, 6432, 8024, 1155, 3209, 5507, 2137, 6656, 5739, 1744, 5300, 1894, 671, 3416, 8024, 738, 6387, 6820, 5543, 2372, 1726, 3341, 671, 702, 5739, 1744, 1957, 3301, 1351, 1450, 511, 102, 4385, 1762, 1920, 3519, 3221, 1525, 671, 2399, 136, 8182, 2399, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1957, 8038, 6814, 1126, 1921, 1155, 3209, 2218, 6206, 794, 5739, 1744, 1726, 3341, 749, 511, 2769, 6820, 4696, 3300, 4157, 1036, 2682, 800, 749, 8024, 6381, 2533, 6929, 2399, 800, 3221, 1157, 6814, 2130, 704, 4904, 5688, 6624, 4638, 511, 4511, 8038, 1377, 679, 3221, 1658, 106, 5632, 794, 2769, 1343, 3189, 3315, 4522, 2110, 8024, 2218, 1086, 738, 3766, 6224, 6814, 800, 8024, 5050, 671, 5050, 6963, 758, 2399, 749, 511, 1957, 8038, 794, 8202, 2399, 2769, 812, 1762, 2110, 3413, 5018, 671, 3613, 6224, 7481, 1168, 4385, 1762, 2347, 5307, 2571, 1282, 2399, 749, 511, 2769, 6820, 4696, 2682, 4692, 4692, 1155, 3209, 1359, 2768, 784, 720, 3416, 749, 106, 4511, 8038, 872, 6820, 1166, 6432, 8024, 1155, 3209, 5507, 2137, 6656, 5739, 1744, 5300, 1894, 671, 3416, 8024, 738, 6387, 6820, 5543, 2372, 1726, 3341, 671, 702, 5739, 1744, 1957, 3301, 1351, 1450, 511, 102, 4385, 1762, 1920, 3519, 3221, 1525, 671, 2399, 136, 8170, 2399, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], 'token_type_ids': [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], 'attention_mask': [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], 'labels': [1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(res[3:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cee0a07-e34b-4b01-9914-2c0efee82447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'context', 'question', 'choice', 'answer', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 11869\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'context', 'question', 'choice', 'answer', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3816\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_c3 = c3.map(process_function, batched=True)\n",
    "tokenized_c3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd8241a-e4a5-4f76-8929-fe0ea9395318",
   "metadata": {},
   "source": [
    "### 4. Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0846e8f-a3d5-4783-b780-ee7f5682686d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at hfl/chinese-macbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMultipleChoice.from_pretrained(\"hfl/chinese-macbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67939dde-3508-4c7c-8c9a-820fd0a20336",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.data = param.data.contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284cb972-9501-4b29-a708-bec344ce46a8",
   "metadata": {},
   "source": [
    "### 5. Create evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5053e681-f496-40ac-b53d-1df5e1585a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c28ef1a3-c518-4882-849c-018b799d5898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = np.argmax(predictions, axis = -1)\n",
    "    return accuracy.compute(predictions = predictions, references = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14567df3-61df-4915-b3e2-f0d6d68caa2f",
   "metadata": {},
   "source": [
    "### 6. Set up training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84a2bc24-9f8e-4a94-a5e6-414c04e79dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir = \"./mcq\",\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    num_train_epochs = 2,\n",
    "    logging_steps = 10,\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    load_best_model_at_end = True,\n",
    "    # fp16 = True,\n",
    "    # bf16=use_mps,\n",
    "    disable_tqdm = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0149f3e-389c-4f5e-969b-fd2ed9cd9fbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "average_tokens_across_devices=False,\n",
       "batch_eval_metrics=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_persistent_workers=False,\n",
       "dataloader_pin_memory=True,\n",
       "dataloader_prefetch_factor=None,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=True,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_do_concat_batches=True,\n",
       "eval_on_start=False,\n",
       "eval_steps=None,\n",
       "eval_strategy=epoch,\n",
       "eval_use_gather_object=False,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "gradient_checkpointing_kwargs=None,\n",
       "greater_is_better=False,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=None,\n",
       "hub_revision=None,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_for_metrics=[],\n",
       "include_inputs_for_metrics=False,\n",
       "include_num_input_tokens_seen=False,\n",
       "include_tokens_per_second=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "liger_kernel_config=None,\n",
       "load_best_model_at_end=True,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=./mcq/runs/Aug14_18-31-11_Tuos-Mac-mini.local,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=10,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_kwargs={},\n",
       "lr_scheduler_type=linear,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=loss,\n",
       "mp_parameters=,\n",
       "neftune_noise_alpha=None,\n",
       "no_cuda=False,\n",
       "num_train_epochs=2,\n",
       "optim=adamw_torch,\n",
       "optim_args=None,\n",
       "optim_target_modules=None,\n",
       "output_dir=./mcq,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=8,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=['tensorboard'],\n",
       "restore_callback_states_from_checkpoint=False,\n",
       "resume_from_checkpoint=None,\n",
       "run_name=./mcq,\n",
       "save_on_each_node=False,\n",
       "save_only_model=False,\n",
       "save_safetensors=True,\n",
       "save_steps=500,\n",
       "save_strategy=epoch,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torch_empty_cache_steps=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_cpu=False,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_liger_kernel=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.0,\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18da6ab3-58f6-4904-bc03-56cf9bf76d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sj/km6fyx_57pn857bnxsqvkj_80000gn/T/ipykernel_92952/2621065752.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = args,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = tokenized_c3[\"train\"].select(range(3000)),\n",
    "    eval_dataset = tokenized_c3[\"validation\"].select(range(1000)),\n",
    "    compute_metrics = compute_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2f3c8dc-9b1c-4d8f-9d26-8750fb49bcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Advanced_AI/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/anaconda3/envs/Advanced_AI/lib/python3.10/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4087, 'grad_norm': 3.423954486846924, 'learning_rate': 4.94e-05, 'epoch': 0.02666666666666667}\n",
      "{'loss': 1.3442, 'grad_norm': 9.257855415344238, 'learning_rate': 4.8733333333333337e-05, 'epoch': 0.05333333333333334}\n",
      "{'loss': 1.2155, 'grad_norm': 12.889373779296875, 'learning_rate': 4.806666666666667e-05, 'epoch': 0.08}\n",
      "{'loss': 1.2541, 'grad_norm': 21.959035873413086, 'learning_rate': 4.74e-05, 'epoch': 0.10666666666666667}\n",
      "{'loss': 1.3318, 'grad_norm': 10.507397651672363, 'learning_rate': 4.6733333333333335e-05, 'epoch': 0.13333333333333333}\n",
      "{'loss': 1.2537, 'grad_norm': 21.981111526489258, 'learning_rate': 4.606666666666667e-05, 'epoch': 0.16}\n",
      "{'loss': 1.1767, 'grad_norm': 14.444607734680176, 'learning_rate': 4.5400000000000006e-05, 'epoch': 0.18666666666666668}\n",
      "{'loss': 1.1926, 'grad_norm': 18.59183120727539, 'learning_rate': 4.473333333333334e-05, 'epoch': 0.21333333333333335}\n",
      "{'loss': 1.2286, 'grad_norm': 12.094952583312988, 'learning_rate': 4.406666666666667e-05, 'epoch': 0.24}\n",
      "{'loss': 1.1582, 'grad_norm': 13.112288475036621, 'learning_rate': 4.3400000000000005e-05, 'epoch': 0.26666666666666666}\n",
      "{'loss': 1.3344, 'grad_norm': 32.737457275390625, 'learning_rate': 4.273333333333333e-05, 'epoch': 0.29333333333333333}\n",
      "{'loss': 1.2852, 'grad_norm': 8.103901863098145, 'learning_rate': 4.206666666666667e-05, 'epoch': 0.32}\n",
      "{'loss': 1.1644, 'grad_norm': 42.891998291015625, 'learning_rate': 4.14e-05, 'epoch': 0.3466666666666667}\n",
      "{'loss': 1.1961, 'grad_norm': 13.412981986999512, 'learning_rate': 4.073333333333333e-05, 'epoch': 0.37333333333333335}\n",
      "{'loss': 1.1692, 'grad_norm': 11.137763023376465, 'learning_rate': 4.006666666666667e-05, 'epoch': 0.4}\n",
      "{'loss': 1.209, 'grad_norm': 28.12099838256836, 'learning_rate': 3.94e-05, 'epoch': 0.4266666666666667}\n",
      "{'loss': 1.1566, 'grad_norm': 12.653714179992676, 'learning_rate': 3.873333333333333e-05, 'epoch': 0.4533333333333333}\n",
      "{'loss': 1.0281, 'grad_norm': 9.124993324279785, 'learning_rate': 3.8066666666666666e-05, 'epoch': 0.48}\n",
      "{'loss': 1.2001, 'grad_norm': 31.207387924194336, 'learning_rate': 3.74e-05, 'epoch': 0.5066666666666667}\n",
      "{'loss': 1.2412, 'grad_norm': 16.983592987060547, 'learning_rate': 3.6733333333333336e-05, 'epoch': 0.5333333333333333}\n",
      "{'loss': 1.1568, 'grad_norm': 8.299649238586426, 'learning_rate': 3.606666666666667e-05, 'epoch': 0.56}\n",
      "{'loss': 1.1325, 'grad_norm': 17.02054214477539, 'learning_rate': 3.54e-05, 'epoch': 0.5866666666666667}\n",
      "{'loss': 1.1388, 'grad_norm': 19.995649337768555, 'learning_rate': 3.4733333333333335e-05, 'epoch': 0.6133333333333333}\n",
      "{'loss': 1.0216, 'grad_norm': 13.817948341369629, 'learning_rate': 3.406666666666667e-05, 'epoch': 0.64}\n",
      "{'loss': 1.0569, 'grad_norm': 21.71444320678711, 'learning_rate': 3.3400000000000005e-05, 'epoch': 0.6666666666666666}\n",
      "{'loss': 1.2104, 'grad_norm': 8.394721984863281, 'learning_rate': 3.2733333333333334e-05, 'epoch': 0.6933333333333334}\n",
      "{'loss': 1.1886, 'grad_norm': 17.73194122314453, 'learning_rate': 3.206666666666667e-05, 'epoch': 0.72}\n",
      "{'loss': 1.0483, 'grad_norm': 9.152471542358398, 'learning_rate': 3.1400000000000004e-05, 'epoch': 0.7466666666666667}\n",
      "{'loss': 1.1303, 'grad_norm': 15.50490951538086, 'learning_rate': 3.073333333333334e-05, 'epoch': 0.7733333333333333}\n",
      "{'loss': 1.1902, 'grad_norm': 7.9836344718933105, 'learning_rate': 3.006666666666667e-05, 'epoch': 0.8}\n",
      "{'loss': 1.0027, 'grad_norm': 20.080318450927734, 'learning_rate': 2.94e-05, 'epoch': 0.8266666666666667}\n",
      "{'loss': 1.0927, 'grad_norm': 32.95941162109375, 'learning_rate': 2.8733333333333335e-05, 'epoch': 0.8533333333333334}\n",
      "{'loss': 1.4444, 'grad_norm': 7.839505672454834, 'learning_rate': 2.806666666666667e-05, 'epoch': 0.88}\n",
      "{'loss': 0.9985, 'grad_norm': 11.837136268615723, 'learning_rate': 2.7400000000000002e-05, 'epoch': 0.9066666666666666}\n",
      "{'loss': 1.0112, 'grad_norm': 20.816165924072266, 'learning_rate': 2.6733333333333334e-05, 'epoch': 0.9333333333333333}\n",
      "{'loss': 1.1083, 'grad_norm': 10.033910751342773, 'learning_rate': 2.6066666666666666e-05, 'epoch': 0.96}\n",
      "{'loss': 0.9682, 'grad_norm': 25.49239730834961, 'learning_rate': 2.54e-05, 'epoch': 0.9866666666666667}\n",
      "{'eval_loss': 1.0902912616729736, 'eval_accuracy': 0.549, 'eval_runtime': 92.8647, 'eval_samples_per_second': 10.768, 'eval_steps_per_second': 1.346, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Advanced_AI/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/anaconda3/envs/Advanced_AI/lib/python3.10/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9445, 'grad_norm': 19.225492477416992, 'learning_rate': 2.4733333333333333e-05, 'epoch': 1.0133333333333334}\n",
      "{'loss': 0.8057, 'grad_norm': 33.72003173828125, 'learning_rate': 2.4066666666666668e-05, 'epoch': 1.04}\n",
      "{'loss': 0.7116, 'grad_norm': 33.935279846191406, 'learning_rate': 2.3400000000000003e-05, 'epoch': 1.0666666666666667}\n",
      "{'loss': 1.0463, 'grad_norm': 21.617469787597656, 'learning_rate': 2.2733333333333335e-05, 'epoch': 1.0933333333333333}\n",
      "{'loss': 0.748, 'grad_norm': 29.042076110839844, 'learning_rate': 2.206666666666667e-05, 'epoch': 1.12}\n",
      "{'loss': 0.7569, 'grad_norm': 16.382770538330078, 'learning_rate': 2.1400000000000002e-05, 'epoch': 1.1466666666666667}\n",
      "{'loss': 0.7629, 'grad_norm': 16.872390747070312, 'learning_rate': 2.0733333333333334e-05, 'epoch': 1.1733333333333333}\n",
      "{'loss': 0.8266, 'grad_norm': 10.434231758117676, 'learning_rate': 2.0066666666666665e-05, 'epoch': 1.2}\n",
      "{'loss': 0.8099, 'grad_norm': 14.975920677185059, 'learning_rate': 1.94e-05, 'epoch': 1.2266666666666666}\n",
      "{'loss': 0.8308, 'grad_norm': 31.63896369934082, 'learning_rate': 1.8733333333333332e-05, 'epoch': 1.2533333333333334}\n",
      "{'loss': 0.7072, 'grad_norm': 15.636855125427246, 'learning_rate': 1.8066666666666668e-05, 'epoch': 1.28}\n",
      "{'loss': 0.9766, 'grad_norm': 21.55813980102539, 'learning_rate': 1.74e-05, 'epoch': 1.3066666666666666}\n",
      "{'loss': 0.6219, 'grad_norm': 24.47797203063965, 'learning_rate': 1.6733333333333335e-05, 'epoch': 1.3333333333333333}\n",
      "{'loss': 0.7311, 'grad_norm': 28.661500930786133, 'learning_rate': 1.606666666666667e-05, 'epoch': 1.3599999999999999}\n",
      "{'loss': 0.7612, 'grad_norm': 21.115732192993164, 'learning_rate': 1.54e-05, 'epoch': 1.3866666666666667}\n",
      "{'loss': 0.7117, 'grad_norm': 41.205162048339844, 'learning_rate': 1.4733333333333335e-05, 'epoch': 1.4133333333333333}\n",
      "{'loss': 0.6064, 'grad_norm': 27.907888412475586, 'learning_rate': 1.4066666666666667e-05, 'epoch': 1.44}\n",
      "{'loss': 0.7994, 'grad_norm': 17.473148345947266, 'learning_rate': 1.3400000000000002e-05, 'epoch': 1.4666666666666668}\n",
      "{'loss': 0.6162, 'grad_norm': 11.15581226348877, 'learning_rate': 1.2733333333333334e-05, 'epoch': 1.4933333333333334}\n",
      "{'loss': 0.7226, 'grad_norm': 24.406511306762695, 'learning_rate': 1.2066666666666667e-05, 'epoch': 1.52}\n",
      "{'loss': 0.6238, 'grad_norm': 33.970008850097656, 'learning_rate': 1.1400000000000001e-05, 'epoch': 1.5466666666666666}\n",
      "{'loss': 0.9526, 'grad_norm': 31.891368865966797, 'learning_rate': 1.0733333333333334e-05, 'epoch': 1.5733333333333333}\n",
      "{'loss': 0.7321, 'grad_norm': 25.367176055908203, 'learning_rate': 1.0066666666666668e-05, 'epoch': 1.6}\n",
      "{'loss': 0.6721, 'grad_norm': 23.609268188476562, 'learning_rate': 9.4e-06, 'epoch': 1.6266666666666667}\n",
      "{'loss': 0.6845, 'grad_norm': 20.058794021606445, 'learning_rate': 8.733333333333333e-06, 'epoch': 1.6533333333333333}\n",
      "{'loss': 0.7329, 'grad_norm': 31.67412567138672, 'learning_rate': 8.066666666666667e-06, 'epoch': 1.6800000000000002}\n",
      "{'loss': 0.5739, 'grad_norm': 18.750267028808594, 'learning_rate': 7.4e-06, 'epoch': 1.7066666666666666}\n",
      "{'loss': 0.7028, 'grad_norm': 16.67905616760254, 'learning_rate': 6.733333333333333e-06, 'epoch': 1.7333333333333334}\n",
      "{'loss': 0.8392, 'grad_norm': 16.7598934173584, 'learning_rate': 6.066666666666667e-06, 'epoch': 1.76}\n",
      "{'loss': 0.7219, 'grad_norm': 18.99032211303711, 'learning_rate': 5.4e-06, 'epoch': 1.7866666666666666}\n",
      "{'loss': 0.7533, 'grad_norm': 12.258362770080566, 'learning_rate': 4.7333333333333335e-06, 'epoch': 1.8133333333333335}\n",
      "{'loss': 0.5348, 'grad_norm': 11.548788070678711, 'learning_rate': 4.066666666666666e-06, 'epoch': 1.8399999999999999}\n",
      "{'loss': 0.6373, 'grad_norm': 21.289854049682617, 'learning_rate': 3.4000000000000005e-06, 'epoch': 1.8666666666666667}\n",
      "{'loss': 0.747, 'grad_norm': 22.93274688720703, 'learning_rate': 2.7333333333333336e-06, 'epoch': 1.8933333333333333}\n",
      "{'loss': 0.5511, 'grad_norm': 9.06820297241211, 'learning_rate': 2.0666666666666666e-06, 'epoch': 1.92}\n",
      "{'loss': 0.621, 'grad_norm': 27.632158279418945, 'learning_rate': 1.4000000000000001e-06, 'epoch': 1.9466666666666668}\n",
      "{'loss': 0.726, 'grad_norm': 18.732486724853516, 'learning_rate': 7.333333333333333e-07, 'epoch': 1.9733333333333334}\n",
      "{'loss': 0.7592, 'grad_norm': 28.244731903076172, 'learning_rate': 6.666666666666667e-08, 'epoch': 2.0}\n",
      "{'eval_loss': 1.1102001667022705, 'eval_accuracy': 0.577, 'eval_runtime': 91.4895, 'eval_samples_per_second': 10.93, 'eval_steps_per_second': 1.366, 'epoch': 2.0}\n",
      "{'train_runtime': 3565.2596, 'train_samples_per_second': 1.683, 'train_steps_per_second': 0.21, 'train_loss': 0.9534867680867513, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=0.9534867680867513, metrics={'train_runtime': 3565.2596, 'train_samples_per_second': 1.683, 'train_steps_per_second': 0.21, 'train_loss': 0.9534867680867513, 'epoch': 2.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c2632-341a-42a3-848e-134fae9803eb",
   "metadata": {},
   "source": [
    "### 9. Model prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c0ffc-975f-46bd-b65d-db6dcf2d8fed",
   "metadata": {},
   "source": [
    "As we still need to process the test data, so we just create a class to incorporate all the procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3411234-1631-48ef-bac2-d44c381b0087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76dd3852-93ad-4b24-a378-21b2e97cd04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleChoicePipeline:\n",
    "\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = model.device\n",
    "\n",
    "    def preprocess(self, context, question, choices):\n",
    "        ctx = [] #sentence1\n",
    "        question_choice = [] #sentence2\n",
    "        for choice in choices:\n",
    "            ctx.append(context)\n",
    "            question_choice.append(question + \" \" + choice)\n",
    "\n",
    "        print(f\"ctx is {ctx}, \\nquestion_choice is {question_choice}\")\n",
    "        return tokenizer(ctx, question_choice, truncation = \"only_first\", max_length = 256, return_tensors = \"pt\", padding=True)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        print(f\"inputs is {inputs}\")\n",
    "        \"\"\"\n",
    "         Hugging Face multiple-choice models expect the input tensors to have three dimensions:\n",
    "         batch_size, num_choices, seq_length\n",
    "         If you skip unsqueeze(0), the model will think you're passing 4 separate examples, each with one choice,\n",
    "         instead of 1 example with 4 choices.\n",
    "         That breaks the logic of multiple-choice classification, where the model compares all choices within a single example.\n",
    "        \"\"\"\n",
    "        inputs = {k: v.unsqueeze(0).to(self.device) for k, v in inputs.items()}\n",
    "        print(f\"After quick process, inputs is {inputs}\")\n",
    "        return self.model(**inputs).logits\n",
    "\n",
    "    def postprocess(self, logits, choices):\n",
    "        prediction = torch.argmax(logits, dim = -1).cpu().item()\n",
    "        return choices[prediction]\n",
    "\n",
    "    def __call__(self, context, question, choices):\n",
    "        inputs = self.preprocess(context, question, choices)\n",
    "        logits = self.predict(inputs)\n",
    "        result = self.postprocess(logits, choices)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c40c837-03b3-4f7b-9ff3-216f131eb54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = MultipleChoicePipeline(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc8246f9-7b46-4151-87b6-7fe7e3981435",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_test = \"成功依靠机遇，英雄出自挑战。机，就是命运给予我们的机会；遇，就是我们去闯、去感受、去把握。命运是公平的，每棵草上都有一颗露珠。在人生路上，我们会有很多次机遇，或当官、或发财、或发明创造……抓住一个，你就会成功。问题是，我们绝大部分的人总是与机遇擦肩而过，或者在机遇面前只是想想而没有采取行动。到最后，人们总是抱怨命运不公平，认为自己没有出生在一个好的时代。有机遇就有挑战。我们一出生便面临各种挑战：学习走路时，我们无数次地摔倒，却仍然扶养墙站起来继续走；学话时，即使咬到舌头，一个字读了几千遍，也要把音念准；上学后，我们一心一意地读书，十多年如一日，放弃了自己的兴趣和爱好……挑战就是我们遇到的种种困难，我们无法回避，也不应该回避。每战胜一个困难，我们就前进一步，而新的困难又会拦在我们前进的路上，我们别无选择，只有迎难而上。\"\n",
    "question_test = \"我们一生中会有多少次机遇？\"\n",
    "choices_test = [\"一次\",\"许多次\",\"没有\",\"四次\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28c328e9-ec8c-4055-b93a-cd797b4e827b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctx is ['成功依靠机遇，英雄出自挑战。机，就是命运给予我们的机会；遇，就是我们去闯、去感受、去把握。命运是公平的，每棵草上都有一颗露珠。在人生路上，我们会有很多次机遇，或当官、或发财、或发明创造……抓住一个，你就会成功。问题是，我们绝大部分的人总是与机遇擦肩而过，或者在机遇面前只是想想而没有采取行动。到最后，人们总是抱怨命运不公平，认为自己没有出生在一个好的时代。有机遇就有挑战。我们一出生便面临各种挑战：学习走路时，我们无数次地摔倒，却仍然扶养墙站起来继续走；学话时，即使咬到舌头，一个字读了几千遍，也要把音念准；上学后，我们一心一意地读书，十多年如一日，放弃了自己的兴趣和爱好……挑战就是我们遇到的种种困难，我们无法回避，也不应该回避。每战胜一个困难，我们就前进一步，而新的困难又会拦在我们前进的路上，我们别无选择，只有迎难而上。', '成功依靠机遇，英雄出自挑战。机，就是命运给予我们的机会；遇，就是我们去闯、去感受、去把握。命运是公平的，每棵草上都有一颗露珠。在人生路上，我们会有很多次机遇，或当官、或发财、或发明创造……抓住一个，你就会成功。问题是，我们绝大部分的人总是与机遇擦肩而过，或者在机遇面前只是想想而没有采取行动。到最后，人们总是抱怨命运不公平，认为自己没有出生在一个好的时代。有机遇就有挑战。我们一出生便面临各种挑战：学习走路时，我们无数次地摔倒，却仍然扶养墙站起来继续走；学话时，即使咬到舌头，一个字读了几千遍，也要把音念准；上学后，我们一心一意地读书，十多年如一日，放弃了自己的兴趣和爱好……挑战就是我们遇到的种种困难，我们无法回避，也不应该回避。每战胜一个困难，我们就前进一步，而新的困难又会拦在我们前进的路上，我们别无选择，只有迎难而上。', '成功依靠机遇，英雄出自挑战。机，就是命运给予我们的机会；遇，就是我们去闯、去感受、去把握。命运是公平的，每棵草上都有一颗露珠。在人生路上，我们会有很多次机遇，或当官、或发财、或发明创造……抓住一个，你就会成功。问题是，我们绝大部分的人总是与机遇擦肩而过，或者在机遇面前只是想想而没有采取行动。到最后，人们总是抱怨命运不公平，认为自己没有出生在一个好的时代。有机遇就有挑战。我们一出生便面临各种挑战：学习走路时，我们无数次地摔倒，却仍然扶养墙站起来继续走；学话时，即使咬到舌头，一个字读了几千遍，也要把音念准；上学后，我们一心一意地读书，十多年如一日，放弃了自己的兴趣和爱好……挑战就是我们遇到的种种困难，我们无法回避，也不应该回避。每战胜一个困难，我们就前进一步，而新的困难又会拦在我们前进的路上，我们别无选择，只有迎难而上。', '成功依靠机遇，英雄出自挑战。机，就是命运给予我们的机会；遇，就是我们去闯、去感受、去把握。命运是公平的，每棵草上都有一颗露珠。在人生路上，我们会有很多次机遇，或当官、或发财、或发明创造……抓住一个，你就会成功。问题是，我们绝大部分的人总是与机遇擦肩而过，或者在机遇面前只是想想而没有采取行动。到最后，人们总是抱怨命运不公平，认为自己没有出生在一个好的时代。有机遇就有挑战。我们一出生便面临各种挑战：学习走路时，我们无数次地摔倒，却仍然扶养墙站起来继续走；学话时，即使咬到舌头，一个字读了几千遍，也要把音念准；上学后，我们一心一意地读书，十多年如一日，放弃了自己的兴趣和爱好……挑战就是我们遇到的种种困难，我们无法回避，也不应该回避。每战胜一个困难，我们就前进一步，而新的困难又会拦在我们前进的路上，我们别无选择，只有迎难而上。'], \n",
      "question_choice is ['我们一生中会有多少次机遇？ 一次', '我们一生中会有多少次机遇？ 许多次', '我们一生中会有多少次机遇？ 没有', '我们一生中会有多少次机遇？ 四次']\n",
      "inputs is {'input_ids': tensor([[ 101, 2768, 1216,  ...,  671, 3613,  102],\n",
      "        [ 101, 2768, 1216,  ..., 1914, 3613,  102],\n",
      "        [ 101, 2768, 1216,  ..., 3766, 3300,  102],\n",
      "        [ 101, 2768, 1216,  ..., 1724, 3613,  102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "After quick process, inputs is {'input_ids': tensor([[[ 101, 2768, 1216,  ...,  671, 3613,  102],\n",
      "         [ 101, 2768, 1216,  ..., 1914, 3613,  102],\n",
      "         [ 101, 2768, 1216,  ..., 3766, 3300,  102],\n",
      "         [ 101, 2768, 1216,  ..., 1724, 3613,  102]]], device='mps:0'), 'token_type_ids': tensor([[[0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1]]], device='mps:0'), 'attention_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1]]], device='mps:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Advanced_AI/lib/python3.10/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'没有'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(context_test, question_test, choices_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e62f4f4-5ba2-454d-8a8e-6e5d1f229c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_test = \"这几年公司发展得很不错，每年春节前都会发给工人两个月的奖金，但是今年公司却没挣到多少钱。经理很担心工人们会伤心、失望。这天，他突然想起小时候去买糖：别的服务员都是先抓一大把，拿去称，再一颗一颗减少；只有一个服务员，每次都抓不够重量，然后一颗一颗往上加。虽然拿到的糖是一样的，但人们都喜欢后者。经理想到了办法。过了两天，传来一个消息——今年公司发展不好，有些人可能得离开公司。工人们听了之后都开始担心，以为要离开的是自己。后来经理宣布了一个消息：大家都是一家人，虽然公司有困难，但不能丢掉任何人，只是没有奖金了。这个消息使所有的人都放下了心：奖金不重要，有工作就好。春节快到了，工人们都做了过个穷年的打算。这时经理通知开会，工人们又担心：“会有什么变化吗？”谁知参加会议的人回来兴奋地喊道：“有！有！还是有奖金的！一个月的！”工人们听了，发出一片热烈的欢呼声。\"\n",
    "question_test = \"关于经理的办法，正确的是？\"\n",
    "choices_test = [\n",
    "\"让工人都很难过\",\n",
    "\"增加了公司的收入\",\n",
    "\"使工人得到了安慰\",\n",
    "\"使一部分人丢了工作\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de25c19b-133a-4471-9f32-7eaa95ead536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctx is ['这几年公司发展得很不错，每年春节前都会发给工人两个月的奖金，但是今年公司却没挣到多少钱。经理很担心工人们会伤心、失望。这天，他突然想起小时候去买糖：别的服务员都是先抓一大把，拿去称，再一颗一颗减少；只有一个服务员，每次都抓不够重量，然后一颗一颗往上加。虽然拿到的糖是一样的，但人们都喜欢后者。经理想到了办法。过了两天，传来一个消息——今年公司发展不好，有些人可能得离开公司。工人们听了之后都开始担心，以为要离开的是自己。后来经理宣布了一个消息：大家都是一家人，虽然公司有困难，但不能丢掉任何人，只是没有奖金了。这个消息使所有的人都放下了心：奖金不重要，有工作就好。春节快到了，工人们都做了过个穷年的打算。这时经理通知开会，工人们又担心：“会有什么变化吗？”谁知参加会议的人回来兴奋地喊道：“有！有！还是有奖金的！一个月的！”工人们听了，发出一片热烈的欢呼声。', '这几年公司发展得很不错，每年春节前都会发给工人两个月的奖金，但是今年公司却没挣到多少钱。经理很担心工人们会伤心、失望。这天，他突然想起小时候去买糖：别的服务员都是先抓一大把，拿去称，再一颗一颗减少；只有一个服务员，每次都抓不够重量，然后一颗一颗往上加。虽然拿到的糖是一样的，但人们都喜欢后者。经理想到了办法。过了两天，传来一个消息——今年公司发展不好，有些人可能得离开公司。工人们听了之后都开始担心，以为要离开的是自己。后来经理宣布了一个消息：大家都是一家人，虽然公司有困难，但不能丢掉任何人，只是没有奖金了。这个消息使所有的人都放下了心：奖金不重要，有工作就好。春节快到了，工人们都做了过个穷年的打算。这时经理通知开会，工人们又担心：“会有什么变化吗？”谁知参加会议的人回来兴奋地喊道：“有！有！还是有奖金的！一个月的！”工人们听了，发出一片热烈的欢呼声。', '这几年公司发展得很不错，每年春节前都会发给工人两个月的奖金，但是今年公司却没挣到多少钱。经理很担心工人们会伤心、失望。这天，他突然想起小时候去买糖：别的服务员都是先抓一大把，拿去称，再一颗一颗减少；只有一个服务员，每次都抓不够重量，然后一颗一颗往上加。虽然拿到的糖是一样的，但人们都喜欢后者。经理想到了办法。过了两天，传来一个消息——今年公司发展不好，有些人可能得离开公司。工人们听了之后都开始担心，以为要离开的是自己。后来经理宣布了一个消息：大家都是一家人，虽然公司有困难，但不能丢掉任何人，只是没有奖金了。这个消息使所有的人都放下了心：奖金不重要，有工作就好。春节快到了，工人们都做了过个穷年的打算。这时经理通知开会，工人们又担心：“会有什么变化吗？”谁知参加会议的人回来兴奋地喊道：“有！有！还是有奖金的！一个月的！”工人们听了，发出一片热烈的欢呼声。', '这几年公司发展得很不错，每年春节前都会发给工人两个月的奖金，但是今年公司却没挣到多少钱。经理很担心工人们会伤心、失望。这天，他突然想起小时候去买糖：别的服务员都是先抓一大把，拿去称，再一颗一颗减少；只有一个服务员，每次都抓不够重量，然后一颗一颗往上加。虽然拿到的糖是一样的，但人们都喜欢后者。经理想到了办法。过了两天，传来一个消息——今年公司发展不好，有些人可能得离开公司。工人们听了之后都开始担心，以为要离开的是自己。后来经理宣布了一个消息：大家都是一家人，虽然公司有困难，但不能丢掉任何人，只是没有奖金了。这个消息使所有的人都放下了心：奖金不重要，有工作就好。春节快到了，工人们都做了过个穷年的打算。这时经理通知开会，工人们又担心：“会有什么变化吗？”谁知参加会议的人回来兴奋地喊道：“有！有！还是有奖金的！一个月的！”工人们听了，发出一片热烈的欢呼声。'], \n",
      "question_choice is ['关于经理的办法，正确的是？ 让工人都很难过', '关于经理的办法，正确的是？ 增加了公司的收入', '关于经理的办法，正确的是？ 使工人得到了安慰', '关于经理的办法，正确的是？ 使一部分人丢了工作']\n",
      "inputs is {'input_ids': tensor([[ 101, 6821, 1126,  ..., 7410, 6814,  102],\n",
      "        [ 101, 6821, 1126,  ..., 3119, 1057,  102],\n",
      "        [ 101, 6821, 1126,  ..., 2128, 2720,  102],\n",
      "        [ 101, 6821, 1126,  ..., 2339,  868,  102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "After quick process, inputs is {'input_ids': tensor([[[ 101, 6821, 1126,  ..., 7410, 6814,  102],\n",
      "         [ 101, 6821, 1126,  ..., 3119, 1057,  102],\n",
      "         [ 101, 6821, 1126,  ..., 2128, 2720,  102],\n",
      "         [ 101, 6821, 1126,  ..., 2339,  868,  102]]], device='mps:0'), 'token_type_ids': tensor([[[0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1]]], device='mps:0'), 'attention_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1]]], device='mps:0')}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'使工人得到了安慰'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(context_test, question_test, choices_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8f3583-1562-4e91-bd44-75c4fcbc77fe",
   "metadata": {},
   "source": [
    "### Below is a test from a model with 600 samples and 1 epoch in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "256229fd-8295-4cee-adf1-6b891462afed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctx is ['成功依靠机遇', '成功依靠机遇', '成功依靠机遇', '成功依靠机遇'], \n",
      "question_choice is ['我们 一次', '我们 许多次', '我们 没有', '我们 四次']\n",
      "inputs is {'input_ids': tensor([[ 101, 2768, 1216,  898, 7479, 3322, 6878,  102, 2769,  812,  671, 3613,\n",
      "          102,    0],\n",
      "        [ 101, 2768, 1216,  898, 7479, 3322, 6878,  102, 2769,  812, 6387, 1914,\n",
      "         3613,  102],\n",
      "        [ 101, 2768, 1216,  898, 7479, 3322, 6878,  102, 2769,  812, 3766, 3300,\n",
      "          102,    0],\n",
      "        [ 101, 2768, 1216,  898, 7479, 3322, 6878,  102, 2769,  812, 1724, 3613,\n",
      "          102,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}\n",
      "After quick process, inputs is {'input_ids': tensor([[[ 101, 2768, 1216,  898, 7479, 3322, 6878,  102, 2769,  812,  671,\n",
      "          3613,  102,    0],\n",
      "         [ 101, 2768, 1216,  898, 7479, 3322, 6878,  102, 2769,  812, 6387,\n",
      "          1914, 3613,  102],\n",
      "         [ 101, 2768, 1216,  898, 7479, 3322, 6878,  102, 2769,  812, 3766,\n",
      "          3300,  102,    0],\n",
      "         [ 101, 2768, 1216,  898, 7479, 3322, 6878,  102, 2769,  812, 1724,\n",
      "          3613,  102,    0]]], device='mps:0'), 'token_type_ids': tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0]]], device='mps:0'), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]], device='mps:0')}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'一次'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(context_test, question_test, choices_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "104b7f1f-a4f1-4d8c-bb52-c43a2db19775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 成 功 依 靠 机 遇 [SEP] 我 们 一 次 [SEP] [PAD]'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([ 101, 2768, 1216,  898, 7479, 3322, 6878,  102, 2769,  812,  671, 3613,\n",
    "          102,    0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
